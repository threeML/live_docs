{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c0d415",
   "metadata": {},
   "source": [
    "# HAL (HAWC Accelerated Likelihood) plugin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fff064e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import numpy as np\n",
    "np.seterr(all=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2288d3fd",
   "metadata": {},
   "source": [
    "The High-Altitude Water Cherenkov Observatory ([HAWC](https://www.hawc-observatory.org/)) is a ground-based wide-field TeV gamma-ray observatory located in Mexico, scanning about 2/3 of the northern sky every day. It is sensitive to gamma rays in the energy range from hundreds of GeV to hundreds of TeV. In addition to gamma ray-induced air showers, HAWC also detects signals from cosmic-ray induced air showers, which make up the main (and partially irreducible) background. HAWC uses a forward-folding likelihood analysis, similar to the approach that is used e.g. by many Fermi-LAT analyses. Most of HAWC's data are only available to collaboration members. However, HAWC has released several [partial-sky datasets](https://data.hawc-observatory.org/) to the public, and is committed to releasing more in the future. If you are interested in a particular HAWC dataset, you can find contact information for HAWC working group leaders on the linked webpage.\n",
    "\n",
    "The HAL (HAWC accelerated likelihood) plugin for threeML is provided in a separate python package, `hawc_hal`. Before running this example offline, make sure that the `HAL` plugin is installed. The `hawc_hal` package needs `root_numpy` as a dependency. It can be installed as follows (skip the first step if you already have threeML/astromodels installed):\n",
    "\n",
    "```\n",
    "conda create -y --name hal_env -c conda-forge -c threeml numpy scipy matplotlib ipython numba reproject \"astromodels>=2\" \"threeml>=2\" root\n",
    "conda activate hal_env\n",
    "pip install --no-binary :all: root_numpy\n",
    "pip install git+https://github.com/threeml/hawc_hal.git\n",
    "```\n",
    "\n",
    "## Download\n",
    "\n",
    "First, download the 507 day Crab dataset and instrument response file from the HAWC webpage.\n",
    "In case of problems with the code below, the files can be downloaded manually from https://data.hawc-observatory.org/datasets/crab_data/index.php . If the files already exist, the code won't try to overwrite them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861afa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def get_hawc_file( filename, odir = \"./\", overwrite = False ):\n",
    "\n",
    "    if overwrite or not os.path.exists( odir + filename):\n",
    "        url=\"https://data.hawc-observatory.org/datasets/crab_data/public_data/crab_2017/\"\n",
    "\n",
    "        req = requests.get(url+filename, verify=False,stream=True)\n",
    "        req.raw.decode_content = True\n",
    "        with open( odir + filename, 'wb') as f:\n",
    "            shutil.copyfileobj(req.raw, f)    \n",
    "        \n",
    "    return odir + filename\n",
    "        \n",
    "\n",
    "maptree = \"HAWC_9bin_507days_crab_data.hd5\"\n",
    "response = \"HAWC_9bin_507days_crab_response.hd5\"\n",
    "odir = \"./\"\n",
    "\n",
    "\n",
    "maptree = get_hawc_file(maptree, odir)\n",
    "response = get_hawc_file(response, odir)\n",
    "\n",
    "assert( os.path.exists(maptree))\n",
    "assert( os.path.exists(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0001a70",
   "metadata": {},
   "source": [
    "Next, we need to define the region of interest (ROI) and instantiate the HAL plugin. The provided data file is a partial map covering 3˚ around the nominal position of the Crab nebula, so we need to make sure we use the same ROI (or a subset) here.\n",
    "\n",
    "Some parameters of note:\n",
    "\n",
    "`flat_sky_pixels_size=0.1`: HAWC data is provided in counts binned according to the HealPIX scheme, with NSIDE=1024. To reduce computing time, the HAL plugin performs the convolution of the model with the detector response on a rectangular grid on the plane tangent to the center of the ROI. The convolved counts are finally converted back to HealPIX for the calculation of the likelihood. The parameter `flat_sky_pixel_size` of the `HAL` class controls the size of the grid used in the convolution with the detector response. Larger grid spacing reduced computing time, but can make the results less reliable. The grid spacing should not be significantly larger than the HealPix pixel size. Also note that model features smaller than the grid spacing may not contribute to the convolved source image. The default here is 0.1˚\n",
    "\n",
    "`hawc.set_active_measurements(1, 9)`: HAWC divides its data in general *analysis bins*, each labeled with a string. The dataset provided here uses 9 bins labeled with integers from 1 to 9 and binned according to the fraction of PMTs hit by a particular shower, which is correlated to the energy of the primary particle. See [Abeysekara et al., 2017](https://iopscience.iop.org/article/10.3847/1538-4357/aa7555/meta) for details about the meaning of the bins. There are two ways to set the 'active' bins (bins to be considered for the fit) in the `HAL` plugin: `set_active_measurement(bin_id_min=1, bin_id_max=9)` (can only be used with numerical bins) and `set_active_measurement(bin_list=[1,2,3,4,5,6,7,8,9])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ed2abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from hawc_hal import HAL, HealpixConeROI\n",
    "import matplotlib.pyplot as plt\n",
    "from threeML import *\n",
    "silence_warnings()\n",
    "%matplotlib inline\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(context='talk', fscale=1, ticks=True, grid=False)\n",
    "set_threeML_style()\n",
    "\n",
    "\n",
    "\n",
    "# Define the ROI. \n",
    "ra_crab, dec_crab = 83.63,22.02\n",
    "data_radius = 3.0 #in degree\n",
    "model_radius = 8.0 #in degree\n",
    "\n",
    "roi = HealpixConeROI(data_radius=data_radius,\n",
    "                     model_radius=model_radius,\n",
    "                     ra=ra_crab,\n",
    "                     dec=dec_crab)\n",
    "\n",
    "# Instance the plugin\n",
    "hawc = HAL(\"HAWC\",\n",
    "           maptree,\n",
    "           response,\n",
    "           roi,\n",
    "           flat_sky_pixels_size=0.1)\n",
    "\n",
    "# Use from bin 1 to bin 9\n",
    "hawc.set_active_measurements(1, 9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f804570f",
   "metadata": {},
   "source": [
    "## Exploratory analysis\n",
    "\n",
    "Next, let's explore the HAWC data. \n",
    "\n",
    "First, print some information about the ROI and the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6ae2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "hawc.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e5e139",
   "metadata": {},
   "source": [
    "Next, some plots. We can use the `display_stacked_image()` function to visualize the background-subtracted counts, summed over all (active) bins. This function smoothes the counts for plotting and takes the width of the smoothing kernel as an argument. This width must be non-zero.\n",
    "\n",
    "First, let's see what the counts look like without (or very little) smoothing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5753f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = hawc.display_stacked_image(smoothing_kernel_sigma=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81613bc2",
   "metadata": {},
   "source": [
    "What about smoothing it with a smoothing kernel comparable to the PSF? (Note the change in the color scale!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9fe299",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_smooth = hawc.display_stacked_image(smoothing_kernel_sigma=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1335c376",
   "metadata": {},
   "source": [
    "Now, let's see how the data change from bin to bin (feel free to play around with the smoothing radius here)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599b7096",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bin in [1, 5, 9]:\n",
    "    \n",
    "    #set only one active bin\n",
    "    hawc.set_active_measurements(bin, bin)\n",
    "\n",
    "    fig = hawc.display_stacked_image(smoothing_kernel_sigma=0.01)\n",
    "    fig.suptitle(\"Bin {}\".format(bin))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f4d4b2",
   "metadata": {},
   "source": [
    "Smaller bins have more events overall, but also more background, and a larger PSF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df816d1",
   "metadata": {},
   "source": [
    "## Simple model fit\n",
    "\n",
    "Let's set up a simple one-source model with a log-parabolic spectrum. For now, the source position will be fixed to its nominal position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "spectrum = Log_parabola()\n",
    "source = PointSource(\"crab\", ra=ra_crab, dec=dec_crab, spectral_shape=spectrum)\n",
    "\n",
    "spectrum.piv = 7 * u.TeV\n",
    "spectrum.piv.fix = True\n",
    "\n",
    "spectrum.K = 1e-14 / (u.TeV * u.cm ** 2 * u.s)  # norm (in 1/(TeV cm2 s))\n",
    "spectrum.K.bounds = (1e-35, 1e-10) / (u.TeV * u.cm ** 2 * u.s)  # without units energies would be in keV\n",
    "\n",
    "spectrum.alpha = -2.5  # log parabolic alpha (index)\n",
    "spectrum.alpha.bounds = (-4., 2.)\n",
    "\n",
    "spectrum.beta = 0  # log parabolic beta (curvature)\n",
    "spectrum.beta.bounds = (-1., 1.)\n",
    "\n",
    "model = Model(source)\n",
    "\n",
    "model.display(complete=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b83a507",
   "metadata": {},
   "source": [
    "Next, set up the likelihood object and run the fit. Fit results are written to disk so they can be read in again later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57154391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure to re-set the active bins before the fit\n",
    "hawc.set_active_measurements(1, 9)\n",
    "data = DataList(hawc)\n",
    "\n",
    "jl = JointLikelihood(model, data, verbose=False)\n",
    "jl.set_minimizer(\"minuit\")\n",
    "param_df, like_df = jl.fit()\n",
    "\n",
    "results=jl.results\n",
    "results.write_to(\"crab_lp_public_results.fits\", overwrite=True)\n",
    "results.optimized_model.save(\"crab_fit.yml\", overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4933f7",
   "metadata": {},
   "source": [
    "## Assessing the Fit Quality\n",
    "\n",
    "`HAL` and `threeML` provide several ways to assess the quality of the provided model, both quantitatively and qualitatively. \n",
    "\n",
    "For a first visual impression, we can display the model, excess (counts - background), background, and residuals (counts - model - background) for each bin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58085073",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = hawc.display_fit(smoothing_kernel_sigma=0.01,display_colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb41c805",
   "metadata": {},
   "source": [
    "Same, but with smoothing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b8e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = hawc.display_fit(smoothing_kernel_sigma=0.2,display_colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8797be2f",
   "metadata": {},
   "source": [
    "The following plot provides a summary of the images above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089625b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = hawc.display_spectrum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ed2832",
   "metadata": {},
   "source": [
    "The `GoodnessOfFit` module provides a numerical assessment of the goodness of fit, by comparing the best-fit likelihood from the fit to likelihood values seen in \"simulated\" data (background + source model + pseudo-random poissonian fluctuation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ad7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "gof_obj = GoodnessOfFit(jl)\n",
    "gof, data_frame, like_data_frame = gof_obj.by_mc(n_iterations=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bf84a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = gof[\"total\"]\n",
    "\n",
    "print(\"Goodness of fit (p-value:)\", p)\n",
    "print(\"Meaning that {:.1f}% of simulations have a larger (worse) likelihood\".format(100*p) )\n",
    "print(\"and {:.1f}% of simulations have a smaller (better) likelihood than seen in data\".format(100*(1-p) ) )\n",
    "\n",
    "df = like_data_frame.reset_index()\n",
    "df = df[df.level_1 == \"total\"]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(df[\"-log(likelihood)\"], label = \"fluctuated source models\", bins=range(18000,19000,50))\n",
    "ax.axvline(like_df.loc[\"total\",\"-log(likelihood)\"], label = \"fit to data\", color = \"orange\" )\n",
    "ax.xlabel = \"-log(likelihood)\"\n",
    "ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea747c3",
   "metadata": {},
   "source": [
    "Not too bad, but not great 🙃. The model might be improved by freeing the position or choosing another spectral shape. However, note that the detector response file provided here is the one used in the 2017 publication, which has since been superceeded by an updated detector model (only available inside the HAWC collaboration for now). Discrepancies between the simulated and true detector response may also worsen the goodness of fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d796e9b5",
   "metadata": {},
   "source": [
    "## Visualizing the Fit Results\n",
    "\n",
    "Now that we have satisfied ourselves that the best-fit model describes the data reasonably well, let's look at the fit parameters. First, let's print them again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2abe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ea685a",
   "metadata": {},
   "source": [
    "Plot the spectrum using `threeML`'s `plot_spectra` convenience function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0789902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plot_spectra(results,\n",
    "                   ene_min=1.0,\n",
    "                   ene_max=37,\n",
    "                   num_ene=50,\n",
    "                   energy_unit='TeV',\n",
    "                   flux_unit='TeV/(s cm2)',\n",
    "                   subplot = ax)\n",
    "ax.set_xlim(0.8,100)\n",
    "ax.set_ylabel(r\"$E^2\\,dN/dE$ [TeV cm$^{-2}$ s$^{-1}$]\")\n",
    "ax.set_xlabel(\"Energy [TeV]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8469d2",
   "metadata": {},
   "source": [
    "What about visualizing the fit parameter's uncertainties and their correlations? We can use *likelihood profiles* for that. We need to provide the range for the contours. We can derive a reasonable range from the best-fit values and their uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb8e10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_min = {}\n",
    "range_max = {}\n",
    "\n",
    "params = model.free_parameters\n",
    "N_param = len(model.free_parameters.keys() )\n",
    "\n",
    "for name in params:\n",
    "    row = param_df.loc[name]\n",
    "    range_min[name] = row[\"value\"] + 5*row[\"negative_error\"]\n",
    "    range_max[name] = row[\"value\"] + 5*row[\"positive_error\"] \n",
    "        \n",
    "for i in range(0,N_param):\n",
    "    p1 = list(params.keys())[i]\n",
    "    p2 = list(params.keys())[(i+1)%N_param]\n",
    "    \n",
    "    a, b, cc, fig = jl.get_contours(p1, range_min[p1], range_max[p1], 20,\n",
    "                                p2, range_min[p2], range_max[p2], 20 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8a9868",
   "metadata": {},
   "source": [
    "We can see that the index and normalization are essentially uncorrelated (as the pivot energy was chosen accordingly). There is significant correlation between the spectral index and the curvature parameter though."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
